# 学习生活工作日记——Day7(2024.7.28)

今天是周末，昨天睡得比较晚，上午就多睡一会补补觉补充体力，迎接下一周的工作。今天需要把Python的内容结束了，然后把最近睡前看的知识蒸馏的视频的内容整理一下，这几个月发现了一个很好的入睡的办法，找AI论文精读的视频比如李沐/同济子豪兄的，听他们讲论文看大概半小时就会睡着了，然后第2天从你有印象的地方继续看，既能帮助入睡，又能学到一点点东西吧， 比看短视频有营养。

---

### 1. Python

- 多线程

  线程是CPU调度的基本单位，共享进程的内存空间，1个进程中至少包含1个线程；

  由于全局解释器锁GIL的存在，同一时刻只有1个线程可以执行Python代码，因此多线程不适合CPU密集型任务；

  线程间的数据通信需要同步，否则会产生异常或者死锁

######  直接创建Thread对象

```python
# 多进程threading 模块
# 直接创建Thread对象
# New_thread = Thread(target=func, arga=(arg1, arg2,...))

from threading import Thread
import time

 
def task(threadName, number, letter):
    print(f"【线程开始】{threadName}")
    for _ in range(number):
        time.sleep(1)
        current_time = time.strftime('%H:%M:%S', time.localtime())
        print(f"[{current_time}] {threadName} 输出 {letter}")
    print(f"【线程结束】{threadName}")
 
 
thread1 = Thread(target=task, args=("thread_1", 4, "a"))  # 线程1：执行任务打印4个a
thread2 = Thread(target=task, args=("thread_2", 2, "b"))  # 线程2：执行任务打印2个b
 
thread1.start()  # 线程1开始
thread2.start()  # 线程2开始
 
thread1.join()  # 等待线程1结束
thread2.join()  # 等待线程2结束
# Output:
'''
【线程开始】thread_1
【线程开始】thread_2
[15:39:15] thread_1 输出 a
[15:39:15] thread_2 输出 b
[15:39:16] thread_2 输出 b
[15:39:16] thread_1 输出 a
【线程结束】thread_2
[15:39:17] thread_1 输出 a
[15:39:18] thread_1 输出 a
【线程结束】thread_1
'''
```

###### 重写Thread类

重写Thread类能够对run函数进行重载，实现更复杂的线程逻辑

```python
# 重写Thread类
# class myThread(threading.Thread)
import threading
import time


class myThread(threading.Thread):
    def __init__(self, number, letter):
        threading.Thread.__init__(self)
        self.number = number
        self.letter = letter

    def run(self):  # 重载run函数
        print(f"【线程开始】{self.name}")
        task1(self.name, self.number, self.letter)  # 线程要调用的函数
        print("【线程结束】", self.name)
 
    def __del__(self):
        print("【线程销毁释放内存】", self.name)
 
 
def task1(threadName, number, letter):
    m = 0
    while m < number:
        time.sleep(1)
        m += 1
        current_time = time.strftime('%H:%M:%S', time.localtime())
        print(f"[{current_time}] {threadName} 输出 {letter}")
 
# def task2...
# def task3...
 
 
thread1 = myThread(4, "a")  # 创建线程thread1：任务耗时2s
thread2 = myThread(2, "b")  # 创建线程thread2：任务耗时4s
 
thread1.start()  # 启动线程1
thread2.start()  # 启动线程2
 
thread1.join()  # 等待线程1
thread2.join()  # 等待线程2

# Output:
'''
【线程开始】Thread-1
【线程开始】Thread-2
[15:57:44] Thread-2 输出 b
[15:57:44] Thread-1 输出 a
[15:57:45] Thread-1 输出 a
[15:57:45] Thread-2 输出 b
【线程结束】 Thread-2
[15:57:46] Thread-1 输出 a
[15:57:47] Thread-1 输出 a
【线程结束】 Thread-1
【线程销毁释放内存】 Thread-1
【线程销毁释放内存】 Thread-2
''
```

###### 线程间资源共享问题

```python
import time
from threading import Thread

n=0
def task1():
    global n
    for i in range(800000):  # 将n循环加800000
        n += 1
    
def task2():
    global n
    print("n is {}".format(n))  # 访问n
    

if __name__ == '__main__':
    print("这里是主线程")
    # 创建线程对象
    t1 = Thread(target=task1)
    t2 = Thread(target=task2)
    # 启动
    t1.start()
    t2.start()
    time.sleep(0.3)
    print("主线程结束了")
# 线程不安全
# t1线程执行还未结束，t2线程访问全局n
# Output：
'''
n is 456307
主线程结束了
'''
```

###### Lock/RLock

当某个线程访问数据时，先对其加锁，其他线程若想访问数据会被阻塞，直到前1个线程锁释放

递归锁RLock能够被一个线程多次获取，也需要用相同数目的release来释放

死锁：由于多个线程之间竞争资源造成的阻塞

例：递归函数中需要对某个全局变量进行修改，第1层加Lock之后，第2层递归的acquire无法获取到锁，于是第1层在等待第2层结束，而第2层在等待第一层的release

```python
lock = threading.Lock()
# 当未锁定时，acquire将状态改为锁定并返回
# 当锁定时，acquire将阻塞其他线程调用release()
lock.acquire()  # 设置锁
# release只能在锁定下调用，否则引发RuntimeError
lock.release()  # 释放锁
lock = Lock()
lock.acquire()
try:
    # do something
finally：
    lock.release()
------------------------------------------------------------------------------------
import time
from threading import Thread, Lock

lock = Lock()  # 创建锁对象
n=0
def task1():
    global n
    global lock
    lock.acquire()
    try:
        for i in range(800000):
            n += 1
    finally :
        lock.release()
    
def task2():
    global n
    lock.acquire()
    try:
        print("task2: n is {}".format(n))
    finally:
        lock.release()
    

if __name__ == '__main__':
    print("这里是主线程")
    # 创建线程对象
    t1 = Thread(target=task1)
    t2 = Thread(target=task2)
    # 启动
    t1.start()
    t2.start()
    print("main: n is {}".format(n))
    time.sleep(0.3)
    print("主线程结束了")
# Output：
'''
这里是主线程
main: n is 800000
task2: n is 800000
主线程结束了
'''
```

###### 线程池Pool

线程被预先创建放入线程池中，同时处理完当前任务之后并不销毁被安排下一任务，能够避免多次创建线程，节省线程创建和销毁的开销

构造：ThreadPoolExecutor(max_workers)

提交任务：submit/map

关闭线程池：shutdown(wait=True)

获取结果： future.result(timeout=None)

任务完成/取消：done()/cancel()

```python
# concurrent.futures模块
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

def task(n):
    print(f"Starting task {n}")
    time.sleep(2)  
    print(f"Finished task {n}")
    return f"Result of task {n}"

def main():
    # 创建一个包含3个线程的线程池
    with ThreadPoolExecutor(max_workers=3) as executor:
        # 提交多个任务到线程池
        futures = [executor.submit(task, i) for i in range(5)]

        # 获取任务执行结果
        for future in as_completed(futures):
            result = future.result()
            print(result)

if __name__ == "__main__":
    main()
# Output:
'''
Starting task 0
Starting task 1
Starting task 2
Finished task 1
Finished task 2
Finished task 0
Result of task 1
Starting task 3
Result of task 2
Starting task 4
Result of task 0
Finished task 4
Finished task 3
Result of task 4
Result of task 3
'''
```

###### concurrent.futures模块

提供ThreadPoolExecutor/ProcessPoolExecutor实现多线程/多进程

使用方法: 创建Executor对象 --> submit/map提交任务并返回Future对象  --> as_completed按完成循序迭代Future对象  --> future.result()获取任务结果 

---

### 三、算法技能

由于在工作会也要读一些论文或者看一些跟算法相关的视频，所以一些算法的内容会穿插着学习和记录，关于算法的内容不会按照一些可能系统得进行，但是我也会尽量把我不清楚的一些内容都覆盖到，而且也要动手去实现，关于算法的内容，我的师傅跟我说必须要先提高广度每一类的算法都有所了解，然后找某个方向去深入的弄清楚所有细节，提高深度，希望自己能够按照这样进行下去

---

#### 1. 知识蒸馏Knowledge Distillation

- 概念：把高精度/规模大的Teacher模型的知识提取到一个轻量化的Student模型中，轻量化模型更容易部署到实际工程中；

- 作用：提升模型精度，如果当前模型A精度不够，可以训练一个高精度但是参数量更大的Teacher模型B，然后将B模型的知识迁移到A模型，提高A模型的精度；

- 使用Teacher网络预测的soft target作为Student网络的标签，hard target([1 0 0])包含的信息没有soft target([0.7, 0.2, 0.1])多，比如soft target能够表示分类结果更像第2类（P=0.2），而最不像第3类（P=0.1）

- 蒸馏温度T：在softmax函数的分母添加T，增加T能够让softmax的输出结果的差异更小
  $$
  {q_i} = \frac{exp(z_i/T)}{\sum_jexp(z_j/T) }
  $$

- 过程：

  - Teacher模型的输出soft labels和Student模型的soft predictions求Loss作为L1
  - Student模型的hard prediction和ground truth求Loss作为L2
  - L = α * L1 + β * L2

  $$
  
  $$

  

  ![image-20240728235149034](C:\Users\44225\AppData\Roaming\Typora\typora-user-images\image-20240728235149034.png)

---

关于知识蒸馏的概念以及具体操作还是比较好理解的，但是关于知识蒸馏我自己有一个疑惑就是这样的方法是对分类任务模型进行蒸馏，那么对回归/聚类任务怎么蒸馏呢，后续去搜索一下，然后这周的话要自己动手实现一下知识蒸馏的过程